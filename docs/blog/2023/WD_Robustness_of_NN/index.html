<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Wasserstein Distributional Robustness of Neural Networks | Yifan Jiang </title> <meta name="author" content="Yifan Jiang"> <meta name="description" content="A paper accepted for NeurIPS 2023."> <meta name="keywords" content="Yifan Jiang, Oxford, math, Yifan-Jiang, Yifan_Jiang, Yifan, 蒋亦凡"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link rel="apple-touch-icon" sizes="180x180" href="/assets/img/apple-touch-icon.png"> <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicon-32x32.png"> <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicon-16x16.png"> <link rel="manifest" href="/assets/img/site.webmanifest"> <link rel="mask-icon" href="/assets/img/safari-pinned-tab.svg" color="#123262"> <link rel="shortcut icon" href="/assets/img/favicon.ico"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-config" content="/assets/img/browserconfig.xml"> <meta name="theme-color" content="#ffffff"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/autumn.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?4d99e8abb7e73f1c2f32df9023bbae3b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yifanjiang233.github.io/blog/2023/WD_Robustness_of_NN/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5VQEBS4MC3"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5VQEBS4MC3");</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script defer src="/assets/js/mathjax_config.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js"></script> </head> <body> <p> --&gt; <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <d-front-matter> <script async type="text/json">
      {
        "title": "Wasserstein Distributional Robustness of Neural Networks",
        "description": "A paper accepted for NeurIPS 2023.",
        "published": "October 2, 2023",
        "authors": [
          
        ]
      }
    </script> </d-front-matter> </p> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yifan Jiang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">Papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Wasserstein Distributional Robustness of Neural Networks</h1> <p class="post-meta"> October 2, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/distributionally-robust-optimization"> <i class="fas fa-hashtag fa-sm"></i> distributionally robust optimization</a>   <a href="/blog/tag/adversarial-attack"> <i class="fas fa-hashtag fa-sm"></i> adversarial attack</a>     ·   <a href="/blog/category/expositions"> <i class="fas fa-tag fa-sm"></i> expositions</a>   </p> </d-title> <d-article> <div class="box" text="TLDR:"> Using the Wasserstein distributionally robust optimization (W-DRO) framework, we derive the first order adversarial attack, link the adversarial accuracy to the adversarial loss, and investigate the out-of-sample performance for neural networks under <em>distributional</em> threat models. </div> <p>This post serves as an exposition of our recent work. Please refer to <a href="https://arxiv.org/abs/2306.09844" rel="external nofollow noopener" target="_blank">arXiv</a> for a more detailed discussion and to <a href="https://github.com/JanObloj/W-DRO-Adversarial-Methods" rel="external nofollow noopener" target="_blank">GitHub</a> for source codes.</p> <h2 id="background">Background</h2> <p>Deep neural networks have achieved great success in image classification tasks. We denote the feature of an image by $x$ and its class by $y$. A neural network $f_{\theta}$ takes $x$ as input and outputs the likelihood of each class the input could be. We denote $P$ as the data distribution. Then, we can write training of a neural network $f_{\theta}$ as finding the minimizer $\theta^{\star}$</p> \[\inf_{\theta} \E_{P}[J_{\theta}(x,y)] \leadsto \theta^{\star},\] <p>where \(J_{\theta}(x,y)=L(f_{\theta}(x),y)\) for some loss function \(L\).</p> <p>In the seminal paper <d-cite key="GSS15"></d-cite>, however, it pointed out that a well-trained neural network is vulnerable under the adversarial attack, i.e., a delicately designed perturbation on the input image. An example is illustrated as follows: by adding an imperceptible noise to the panda image, a neural network changes its prediction from 57.7% “panda” to 99.9% “gibbon”. The middle image is normalized in order to be visible to human eyes. We stress that the perturbation here is carefully chosen as the gradient sign direction $\mathrm{sgn}(\nabla_x J)$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/WDRobustness/panda2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/WDRobustness/panda2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/WDRobustness/panda2-1400.webp"></source> <img src="/assets/img/WDRobustness/panda2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">A demonstration of adversarial attack via Fast Gradient Sign Method (FGSM) <d-cite key="GSS15"></d-cite>.</figcaption> </figure> <p>Classical literature on adversarial attacks, for example <d-cite key="MMS+18"></d-cite>, focus on the <em>pointwise</em> threat model, where a uniform budget $\delta$ of perturbation is given for each image. To generate adversarial images, we to some extent reverse the training process by maximizing the loss function over input data</p> \[\E_{P}\Bigl[\sup_{\|x-x'\|\leq \delta}J_{\theta}(x',y)\Bigr]\leadsto x^{\star}.\] <p>Here, $\|\cdot\|$ is a norm on the feature space, which could be $l_2$, $l_{\infty}$, etc.</p> <p>A key observation here is that,</p> \[\E_{P}\Bigl[\sup_{\|x-x'\|_s\leq \delta}J_{\theta}(x',y)\Bigr]=\sup_{\mathcal{W}_{\infty}(P,Q)\leq \delta}\E_{Q}\bigl[J_{\theta}(x,y)\bigr],\] <p>where $\mathcal{W}_{\infty}$ is the $\infty$-Wasserstein distance induced by</p> \[\label{eqn-d} d((x_{1},y_{1}),(x_{2},y_{2}))=\|x_{1}-x_{2}\|+\infty\mathbf{1}_{\{y_{1}\neq y_{2}\}}.\] <p>In this sense, finding the adversarial images is equivalent to finding the adversarial image distribution under the $\infty$-Wasserstein distance. This motivates us to investigate the <em>distributional</em> threat model and its associated adversarial attack, given by</p> \[\sup_{\mathcal{W}_{2}(P,Q)\leq \delta}\E_{Q}\bigl[J_{\theta}(x,y)\bigr].\] <p>We quickly remark one main feature of <em>distributional</em> threat models. In contrast to the <em>pointwise</em> threat, the attacker has a greater flexibility and can perturb images close to the decision boundary only slightly while spending more of the attack budget on images farther away from the boundary. This makes the <em>distributional</em> adversarial attack more involved.</p> <p>Such a W-DRO framework, while compelling theoretically, is often numerically intractable. We leverage the W-DRO sensitivity results <d-cite key="BDOW21"></d-cite> to carry out attacks under a <em>distributional</em> threat model. We further give asymptotic certified bounds on the adversarial accuracy which are fast to compute and of first-order accuracy. Finally, we utilize concentration inequality of the empirical measure and derive the out-of-sample performance of neural networks under <em>distribution</em> threat models.</p> <h2 id="setup">Setup</h2> <p>An image is interpreted as a tuple $(x,y)$ where the feature vector $x\in \mathcal{X}=[0,1]^{n}$ encodes the graphic information and $y\in\mathcal{Y}=\{1,\dots,m\}$ denotes the class, or tag, of the image. A distribution of labelled images corresponds to a probability measure $P$ on $\mathcal{X}\times\mathcal{Y}$. Under this setting $P$ could be the empirical measure on a given dataset or an inaccessible “true” distribution on the extended image space.</p> <p>Let $(p,q)$ and $(r,s)$ be two pairs of conjugate indices with $1/p+1/q=1/r+1/s=1.$ As mentioned above, we equip $\mathcal{X}$ with $l_s$ norm and consider the Wasserstein distance $\mathcal{W}_p$ generated by $d$.</p> <p>We write the neural network as a map $f_{\theta}:\mathcal{X}\to \mathbb{R}^{m}$. We denote $S$ the set of images equipped with their labels generated by $f_\theta$, i.e.,</p> \[S=\Bigl\{(x,y)\in \mathcal{X}\times\mathcal{Y}: \arg\max_{1\leq i\leq m} f_{\theta}(x)_{i}=\{y\}\Bigr\}.\] <p>We denote the clean accuracy by \(A=\E_P[\mathbf{1}_S]\) and Wasserstein distributional adversarial accuracy by</p> \[A_{\delta}:=\inf_{\mathcal{W}_p(P,Q)\leq \delta}\E_Q[\mathbf{1}_S].\] <h2 id="main-results">Main Results</h2> <p>We always assume the following two assumptions hold.</p> <div class="asmp"> We assume the map \( (x,y)\mapsto J_{\theta}(x,y) \) is \(\mathsf{L}\)-Lipschitz under \(d\), i.e., \[|J_{\theta}(x_1,y_1)-J_{\theta}(x_2,y_2)|\leq \mathsf{L} d((x_1,y_1),(x_2,y_2)).\] </div> <div class="asmp"> We assume that for any \(Q\in B_{\delta}(P)\) <br> (a) $0&lt;Q(S)&lt;1.$ <br> (b) \( \mathcal{W}_{p}(Q(\cdot|S),P(\cdot|S))+\mathcal{W}_{p}(Q(\cdot|S^{c}),P(\cdot|S^{c}))= o(\delta), \) where \(S^{c}=(\mathcal{X}\times\mathcal{Y})\setminus S\) and the conditional distribution is given by $Q(E|S)=Q(E\cap S)/Q(S)$. </div> <h3 id="first-order-adversarial-attack">First Order Adversarial Attack</h3> <p>To propose WD adversarial attacks, we first introduce two important ingredients : sensitivity of W-DRO and rectified DLR loss.</p> <p>We write</p> \[V(\delta)=\sup_{\mathcal{W}_{p}(P,Q)\leq \delta}\E_{Q}\bigl[J_{\theta}(x,y)\bigr].\] <p>The following theorem is adapted from <d-cite key="BDOW21"></d-cite>.</p> <div class="thm"> Under the above assumptions, we have the following first order approximations hold: <br> (a) $V(\delta)=V(0)+\delta\Upsilon+o(\delta),$ where \begin{equation*} \Upsilon=\Bigl(\E_{P}\|\nabla_{x}J_{\theta}(x,y)\|_{r}^{q}\Bigr)^{1/q}. \end{equation*} (b) \(V(\delta)=\E_{Q_{\delta}}[J_{\theta}(x,y)]+o(\delta),\) where \begin{equation*} Q_{\delta}=\Bigl[(x,y)\mapsto \bigl(x+\delta h(\nabla_{x}J_{\theta}(x,y))\|\Upsilon^{-1}\nabla_{x}J_{\theta}(x,y)\|_{r}^{q-1},y\bigr)\Bigr]_{\#}P, \end{equation*} and \(h\) is uniquely determined by \(\langle h(x),x\rangle=\|x\|_{r}\). </div> <p>The second result essentially says to maxize the loss, the perturbation given by</p> \[x\to x+\delta h(\nabla_{x}J_{\theta}(x,y))\|\Upsilon^{-1}\nabla_{x}J_{\theta}(x,y)\|_{r}^{q-1}\] <p>is first-order optimal. Particularly, if we take $p=\infty$ and $q=\infty$, we retrieve Fast Gradient Descent Method (FGSM) mentioned above for <em>pintwise</em> threat models.</p> <p>Under <em>pointwise</em> threat models, taking loss function $L$ as a combination of Cross Entropy (CE) loss and Difference of Logits Ratio (DLR) loss has been widely shown as an effective empirical attack, see <d-cite key="CH20"></d-cite> for details. The DLR loss is given by</p> \[\mathrm{DLR}(z,y)=\left\{\begin{aligned} -\frac{z_{y}-z_{(2)}}{z_{(1)}-z_{(3)}}, &amp; \quad\text{if } z_{y}=z_{(1)}, \\ -\frac{z_{y}-z_{(1)}}{z_{(1)}-z_{(3)}}, &amp; \quad\text{else,} \end{aligned}\right.\] <p>where we write \(z=(z_{1},\dots,z_{m})=f_{\theta}(x)\) for the output of a neural network, and \(z_{(1)}\geq\dots\geq z_{(m)}\) are the order statistics of \(z\). However, under <em>distributional</em> threat models, intuitively, an effective attack should perturb more aggressively images classified far from the decision boundary and leave the misclassified images unchanged. Consequently, neither CE loss nor DLR loss are appropriate. Instead, we propose to use Rectified DLR (ReDLR) loss as a candidate to employ <em>distributional</em> adversarial attacks, which is simply given by</p> \[\mathop{\mathrm{ReDLR}}=-(\mathop{\mathrm{DLR}})^{-}.\] <p>In the figure below, we compare the adversarial accuracy of robust networks on RobustBench <d-cite key="CAS+21"></d-cite> against pointwise threat models and distributional threat models. We notice a significant drop of the adversarial accuracy even for those neural networks robust against pointwise threat models.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/WDRobustness/acc_shortfall-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/WDRobustness/acc_shortfall-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/WDRobustness/acc_shortfall-1400.webp"></source> <img src="/assets/img/WDRobustness/acc_shortfall.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> Shortfall of WD-adversarial accuracy on CIFAR-10 with different metrics $l_{\infty}$ (left) and $l_{2}$ (right). We testify our proposed attack on all neural networks from RobustBench <d-cite key="CAS+21"></d-cite>. </figcaption> </figure> <h3 id="asymptotically-certified-bound">Asymptotically Certified Bound</h3> <p>We write \(\mathcal{R}_{\delta}:=A_{\delta}/A\) as a metric of robustness, and the adversarial loss condition on the misclassified images as</p> \[W(\delta)=\sup_{Q\in B_{\delta}(P)}\E_{Q}[J_{\theta}(x,y)|S^{c}].\] <p>We note that an upper bound on $\mathcal{R}_\delta$ is given by any adversarial attack. In particular,</p> \[\mathcal{R}_\delta \leq \mathcal{R}^u_\delta:= Q_\delta(S)/A.\] <div class="thm"> Under the above assumptions, we have an asymptotic lower bound as $\delta\to 0$ $$ \mathcal{R}_\delta\geq \frac{W(0)-V(\delta)}{W(0)-V(0)} +o(\delta)=\widetilde{\mathcal{R}}_\delta^l+o(\delta)=\overline{\mathcal{R}}^l_\delta+o(\delta), $$ where the first order approximations are given by $$ \widetilde{\mathcal{R}}_\delta^l=\frac{W(0)-\E_{Q_{\delta}}[J_{\theta}(x,y)]}{W(0)-V(0)}\quad \text{and} \quad \overline{\mathcal{R}}_{\delta}^l=\frac{W(0)-V(0)-\delta\Upsilon}{W(0)-V(0)}. $$ </div> <p>Consequently, \(\mathcal{R}^l_{\delta}=\min\{\widetilde{\mathcal{R}}_\delta^l, \overline{\mathcal{R}}_{\delta}^l\}\) allows us to estimate the model robustness without performing any sophisticated adversarial attack. We plot our proposed bounds against the reference robust metric on CIFAR-100 and ImageNet datasets as below. For CIFAR-10 dataset, we refer to our paper. Notably, the bounds provided here is order of magnitude faster to compute than the reference value $\mathcal{R}_{\delta}$ by using AutoAttack <d-cite key="CH20"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/WDRobustness/cifar100_blog-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/WDRobustness/cifar100_blog-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/WDRobustness/cifar100_blog-1400.webp"></source> <img src="/assets/img/WDRobustness/cifar100_blog.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"><img src="/assets/img/WDRobustness/imagenet_blog.jpg" style="margin-top: 10px;" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption"> $\mathcal{R}^{u}$ &amp; $\mathcal{R}^{l}$ versus $\mathcal{R}$ on CIFAR-100 (top) and ImageNet (bottom). </figcaption> </figure> <h3 id="out-of-sample-performance">Out-of-Sample Performance</h3> <p>Our results on distributionally adversarial robustness translate into bounds for performance of the trained DNN on unseen data. We rely on the concentration inequality of empirical measures. Let us fix $1&lt;p&lt;n/2$. We assume the training set is i.i.d sampled from the true distribution $P$ with size $N$. Then the empirical measure of the training set $\widehat{P}$ is a random measure, and satisfies</p> \[\mathbb{P}(\mathcal{W}_p(\widehat{P},P)\geq \varepsilon)\leq K \exp(-KN\varepsilon^n),\] <p>where $K$ is a constant depending on $p$ and $n$. Thank to W-DRO framework, such an estimate naturally yields the out-of-performance of neural networks under <em>distributional</em> threat models.</p> <div class="thm"> Under above Assumptions, with probability at least \(1-K \exp(-KN\varepsilon^{n})\) we have \begin{equation*} V(\delta)\leq \widehat{V}(\delta) + \varepsilon\sup_{Q\in B_{\delta}^{\star}(\widehat{P})}\Bigl(\E_{Q}\|\nabla_{x}J_{\theta}(x,y)\|_{s}^{q}\Bigr)^{1/q} + o(\varepsilon)\leq \widehat{V}(\delta)+ L\varepsilon \end{equation*} where \(B_{\delta}^{\star}(\widehat{P})=\arg\max_{Q\in B_{\delta}(\widehat{P})}\E_{Q}[J_{\theta}(x,y)]\) and constant \(K\) only depends on $p$ and $n$. </div> <p>We remark that the above results are easily extended to the out-of-sample performance on the test set, via the triangle inequality.</p> <h2 id="conclusion">Conclusion</h2> <p>Our work contributes to the understanding of robustness of DNN classifiers. It also offers a wider viewpoint on the question of robustness and naturally links the questions of adversarial attacks, out-of-sample performance, out-of-distribution performance and Knightian uncertainty. By introducing a first-order Adversarial Attack (AA) algorithm, we not only encompass existing methods like FGSM and PGD but also offer new insights into distributional threat models. We believe our research opens up many avenues for future work, especially training robust neural networks under distributional threat models.</p> </d-article> <d-bibliography src="/assets/bibliography/WDRobustness.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Yifan Jiang (蒋亦凡). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5VQEBS4MC3"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5VQEBS4MC3");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>