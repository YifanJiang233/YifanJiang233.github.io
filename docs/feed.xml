<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://yifanjiang233.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yifanjiang233.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T20:47:11+01:00</updated><id>https://yifanjiang233.github.io/feed.xml</id><title type="html">Yifan Jiang</title><subtitle>Personal website of Yifan Jiang. </subtitle><entry><title type="html">Wasserstein Distributional Robustness of Neural Networks</title><link href="https://yifanjiang233.github.io/blog/2023/WD_Robustness_of_NN/" rel="alternate" type="text/html" title="Wasserstein Distributional Robustness of Neural Networks"/><published>2023-10-02T00:00:00+01:00</published><updated>2023-10-02T00:00:00+01:00</updated><id>https://yifanjiang233.github.io/blog/2023/WD_Robustness_of_NN</id><content type="html" xml:base="https://yifanjiang233.github.io/blog/2023/WD_Robustness_of_NN/"><![CDATA[<div class="box" text="TLDR:"> Using the Wasserstein distributionally robust optimization (W-DRO) framework, we derive the first order adversarial attack, link the adversarial accuracy to the adversarial loss, and investigate the out-of-sample performance for neural networks under <em>distributional</em> threat models. </div> <p>This post serves as an exposition of our recent work. Please refer to <a href="https://arxiv.org/abs/2306.09844">arXiv</a> for a more detailed discussion and to <a href="https://github.com/JanObloj/W-DRO-Adversarial-Methods">GitHub</a> for source codes.</p> <h2 id="background">Background</h2> <p>Deep neural networks have achieved great success in image classification tasks. We denote the feature of an image by $x$ and its class by $y$. A neural network $f_{\theta}$ takes $x$ as input and outputs the likelihood of each class the input could be. We denote $P$ as the data distribution. Then, we can write training of a neural network $f_{\theta}$ as finding the minimizer $\theta^{\star}$</p> \[\inf_{\theta} \E_{P}[J_{\theta}(x,y)] \leadsto \theta^{\star},\] <p>where \(J_{\theta}(x,y)=L(f_{\theta}(x),y)\) for some loss function \(L\).</p> <p>In the seminal paper <d-cite key="GSS15"></d-cite>, however, it pointed out that a well-trained neural network is vulnerable under the adversarial attack, i.e., a delicately designed perturbation on the input image. An example is illustrated as follows: by adding an imperceptible noise to the panda image, a neural network changes its prediction from 57.7% “panda” to 99.9% “gibbon”. The middle image is normalized in order to be visible to human eyes. We stress that the perturbation here is carefully chosen as the gradient sign direction $\mathrm{sgn}(\nabla_x J)$.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/WDRobustness/panda2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/WDRobustness/panda2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/WDRobustness/panda2-1400.webp"/> <img src="/assets/img/WDRobustness/panda2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">A demonstration of adversarial attack via Fast Gradient Sign Method (FGSM) <d-cite key="GSS15"></d-cite>.</figcaption> </figure> <p>Classical literature on adversarial attacks, for example <d-cite key="MMS+18"></d-cite>, focus on the <em>pointwise</em> threat model, where a uniform budget $\delta$ of perturbation is given for each image. To generate adversarial images, we to some extent reverse the training process by maximizing the loss function over input data</p> \[\E_{P}\Bigl[\sup_{\|x-x'\|\leq \delta}J_{\theta}(x',y)\Bigr]\leadsto x^{\star}.\] <p>Here, $\|\cdot\|$ is a norm on the feature space, which could be $l_2$, $l_{\infty}$, etc.</p> <p>A key observation here is that,</p> \[\E_{P}\Bigl[\sup_{\|x-x'\|_s\leq \delta}J_{\theta}(x',y)\Bigr]=\sup_{\mathcal{W}_{\infty}(P,Q)\leq \delta}\E_{Q}\bigl[J_{\theta}(x,y)\bigr],\] <p>where $\mathcal{W}_{\infty}$ is the $\infty$-Wasserstein distance induced by</p> \[\label{eqn-d} d((x_{1},y_{1}),(x_{2},y_{2}))=\|x_{1}-x_{2}\|+\infty\mathbf{1}_{\{y_{1}\neq y_{2}\}}.\] <p>In this sense, finding the adversarial images is equivalent to finding the adversarial image distribution under the $\infty$-Wasserstein distance. This motivates us to investigate the <em>distributional</em> threat model and its associated adversarial attack, given by</p> \[\sup_{\mathcal{W}_{2}(P,Q)\leq \delta}\E_{Q}\bigl[J_{\theta}(x,y)\bigr].\] <p>We quickly remark one main feature of <em>distributional</em> threat models. In contrast to the <em>pointwise</em> threat, the attacker has a greater flexibility and can perturb images close to the decision boundary only slightly while spending more of the attack budget on images farther away from the boundary. This makes the <em>distributional</em> adversarial attack more involved.</p> <p>Such a W-DRO framework, while compelling theoretically, is often numerically intractable. We leverage the W-DRO sensitivity results <d-cite key="BDOW21"></d-cite> to carry out attacks under a <em>distributional</em> threat model. We further give asymptotic certified bounds on the adversarial accuracy which are fast to compute and of first-order accuracy. Finally, we utilize concentration inequality of the empirical measure and derive the out-of-sample performance of neural networks under <em>distribution</em> threat models.</p> <h2 id="setup">Setup</h2> <p>An image is interpreted as a tuple $(x,y)$ where the feature vector $x\in \mathcal{X}=[0,1]^{n}$ encodes the graphic information and $y\in\mathcal{Y}=\{1,\dots,m\}$ denotes the class, or tag, of the image. A distribution of labelled images corresponds to a probability measure $P$ on $\mathcal{X}\times\mathcal{Y}$. Under this setting $P$ could be the empirical measure on a given dataset or an inaccessible “true” distribution on the extended image space.</p> <p>Let $(p,q)$ and $(r,s)$ be two pairs of conjugate indices with $1/p+1/q=1/r+1/s=1.$ As mentioned above, we equip $\mathcal{X}$ with $l_s$ norm and consider the Wasserstein distance $\mathcal{W}_p$ generated by $d$.</p> <p>We write the neural network as a map $f_{\theta}:\mathcal{X}\to \mathbb{R}^{m}$. We denote $S$ the set of images equipped with their labels generated by $f_\theta$, i.e.,</p> \[S=\Bigl\{(x,y)\in \mathcal{X}\times\mathcal{Y}: \arg\max_{1\leq i\leq m} f_{\theta}(x)_{i}=\{y\}\Bigr\}.\] <p>We denote the clean accuracy by \(A=\E_P[\mathbf{1}_S]\) and Wasserstein distributional adversarial accuracy by</p> \[A_{\delta}:=\inf_{\mathcal{W}_p(P,Q)\leq \delta}\E_Q[\mathbf{1}_S].\] <h2 id="main-results">Main Results</h2> <p>We always assume the following two assumptions hold.</p> <div class="asmp"> We assume the map \( (x,y)\mapsto J_{\theta}(x,y) \) is \(\mathsf{L}\)-Lipschitz under \(d\), i.e., \[|J_{\theta}(x_1,y_1)-J_{\theta}(x_2,y_2)|\leq \mathsf{L} d((x_1,y_1),(x_2,y_2)).\] </div> <div class="asmp"> We assume that for any \(Q\in B_{\delta}(P)\) <br/> (a) $0&lt;Q(S)&lt;1.$ <br/> (b) \( \mathcal{W}_{p}(Q(\cdot|S),P(\cdot|S))+\mathcal{W}_{p}(Q(\cdot|S^{c}),P(\cdot|S^{c}))= o(\delta), \) where \(S^{c}=(\mathcal{X}\times\mathcal{Y})\setminus S\) and the conditional distribution is given by $Q(E|S)=Q(E\cap S)/Q(S)$. </div> <h3 id="first-order-adversarial-attack">First Order Adversarial Attack</h3> <p>To propose WD adversarial attacks, we first introduce two important ingredients : sensitivity of W-DRO and rectified DLR loss.</p> <p>We write</p> \[V(\delta)=\sup_{\mathcal{W}_{p}(P,Q)\leq \delta}\E_{Q}\bigl[J_{\theta}(x,y)\bigr].\] <p>The following theorem is adapted from <d-cite key="BDOW21"></d-cite>.</p> <div class="thm"> Under the above assumptions, we have the following first order approximations hold: <br/> (a) $V(\delta)=V(0)+\delta\Upsilon+o(\delta),$ where \begin{equation*} \Upsilon=\Bigl(\E_{P}\|\nabla_{x}J_{\theta}(x,y)\|_{r}^{q}\Bigr)^{1/q}. \end{equation*} (b) \(V(\delta)=\E_{Q_{\delta}}[J_{\theta}(x,y)]+o(\delta),\) where \begin{equation*} Q_{\delta}=\Bigl[(x,y)\mapsto \bigl(x+\delta h(\nabla_{x}J_{\theta}(x,y))\|\Upsilon^{-1}\nabla_{x}J_{\theta}(x,y)\|_{r}^{q-1},y\bigr)\Bigr]_{\#}P, \end{equation*} and \(h\) is uniquely determined by \(\langle h(x),x\rangle=\|x\|_{r}\). </div> <p>The second result essentially says to maxize the loss, the perturbation given by</p> \[x\to x+\delta h(\nabla_{x}J_{\theta}(x,y))\|\Upsilon^{-1}\nabla_{x}J_{\theta}(x,y)\|_{r}^{q-1}\] <p>is first-order optimal. Particularly, if we take $p=\infty$ and $q=\infty$, we retrieve Fast Gradient Descent Method (FGSM) mentioned above for <em>pintwise</em> threat models.</p> <p>Under <em>pointwise</em> threat models, taking loss function $L$ as a combination of Cross Entropy (CE) loss and Difference of Logits Ratio (DLR) loss has been widely shown as an effective empirical attack, see <d-cite key="CH20"></d-cite> for details. The DLR loss is given by</p> \[\mathrm{DLR}(z,y)=\left\{\begin{aligned} -\frac{z_{y}-z_{(2)}}{z_{(1)}-z_{(3)}}, &amp; \quad\text{if } z_{y}=z_{(1)}, \\ -\frac{z_{y}-z_{(1)}}{z_{(1)}-z_{(3)}}, &amp; \quad\text{else,} \end{aligned}\right.\] <p>where we write \(z=(z_{1},\dots,z_{m})=f_{\theta}(x)\) for the output of a neural network, and \(z_{(1)}\geq\dots\geq z_{(m)}\) are the order statistics of \(z\). However, under <em>distributional</em> threat models, intuitively, an effective attack should perturb more aggressively images classified far from the decision boundary and leave the misclassified images unchanged. Consequently, neither CE loss nor DLR loss are appropriate. Instead, we propose to use Rectified DLR (ReDLR) loss as a candidate to employ <em>distributional</em> adversarial attacks, which is simply given by</p> \[\mathop{\mathrm{ReDLR}}=-(\mathop{\mathrm{DLR}})^{-}.\] <p>In the figure below, we compare the adversarial accuracy of robust networks on RobustBench <d-cite key="CAS+21"></d-cite> against pointwise threat models and distributional threat models. We notice a significant drop of the adversarial accuracy even for those neural networks robust against pointwise threat models.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/WDRobustness/acc_shortfall-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/WDRobustness/acc_shortfall-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/WDRobustness/acc_shortfall-1400.webp"/> <img src="/assets/img/WDRobustness/acc_shortfall.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> Shortfall of WD-adversarial accuracy on CIFAR-10 with different metrics $l_{\infty}$ (left) and $l_{2}$ (right). We testify our proposed attack on all neural networks from RobustBench <d-cite key="CAS+21"></d-cite>. </figcaption> </figure> <h3 id="asymptotically-certified-bound">Asymptotically Certified Bound</h3> <p>We write \(\mathcal{R}_{\delta}:=A_{\delta}/A\) as a metric of robustness, and the adversarial loss condition on the misclassified images as</p> \[W(\delta)=\sup_{Q\in B_{\delta}(P)}\E_{Q}[J_{\theta}(x,y)|S^{c}].\] <p>We note that an upper bound on $\mathcal{R}_\delta$ is given by any adversarial attack. In particular,</p> \[\mathcal{R}_\delta \leq \mathcal{R}^u_\delta:= Q_\delta(S)/A.\] <div class="thm"> Under the above assumptions, we have an asymptotic lower bound as $\delta\to 0$ $$ \mathcal{R}_\delta\geq \frac{W(0)-V(\delta)}{W(0)-V(0)} +o(\delta)=\widetilde{\mathcal{R}}_\delta^l+o(\delta)=\overline{\mathcal{R}}^l_\delta+o(\delta), $$ where the first order approximations are given by $$ \widetilde{\mathcal{R}}_\delta^l=\frac{W(0)-\E_{Q_{\delta}}[J_{\theta}(x,y)]}{W(0)-V(0)}\quad \text{and} \quad \overline{\mathcal{R}}_{\delta}^l=\frac{W(0)-V(0)-\delta\Upsilon}{W(0)-V(0)}. $$ </div> <p>Consequently, \(\mathcal{R}^l_{\delta}=\min\{\widetilde{\mathcal{R}}_\delta^l, \overline{\mathcal{R}}_{\delta}^l\}\) allows us to estimate the model robustness without performing any sophisticated adversarial attack. We plot our proposed bounds against the reference robust metric on CIFAR-100 and ImageNet datasets as below. For CIFAR-10 dataset, we refer to our paper. Notably, the bounds provided here is order of magnitude faster to compute than the reference value $\mathcal{R}_{\delta}$ by using AutoAttack <d-cite key="CH20"></d-cite>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/WDRobustness/cifar100_blog-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/WDRobustness/cifar100_blog-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/WDRobustness/cifar100_blog-1400.webp"/> <img src="/assets/img/WDRobustness/cifar100_blog.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/><img src="/assets/img/WDRobustness/imagenet_blog.jpg" style="margin-top: 10px;" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"> $\mathcal{R}^{u}$ &amp; $\mathcal{R}^{l}$ versus $\mathcal{R}$ on CIFAR-100 (top) and ImageNet (bottom). </figcaption> </figure> <h3 id="out-of-sample-performance">Out-of-Sample Performance</h3> <p>Our results on distributionally adversarial robustness translate into bounds for performance of the trained DNN on unseen data. We rely on the concentration inequality of empirical measures. Let us fix $1&lt;p&lt;n/2$. We assume the training set is i.i.d sampled from the true distribution $P$ with size $N$. Then the empirical measure of the training set $\widehat{P}$ is a random measure, and satisfies</p> \[\mathbb{P}(\mathcal{W}_p(\widehat{P},P)\geq \varepsilon)\leq K \exp(-KN\varepsilon^n),\] <p>where $K$ is a constant depending on $p$ and $n$. Thank to W-DRO framework, such an estimate naturally yields the out-of-performance of neural networks under <em>distributional</em> threat models.</p> <div class="thm"> Under above Assumptions, with probability at least \(1-K \exp(-KN\varepsilon^{n})\) we have \begin{equation*} V(\delta)\leq \widehat{V}(\delta) + \varepsilon\sup_{Q\in B_{\delta}^{\star}(\widehat{P})}\Bigl(\E_{Q}\|\nabla_{x}J_{\theta}(x,y)\|_{s}^{q}\Bigr)^{1/q} + o(\varepsilon)\leq \widehat{V}(\delta)+ L\varepsilon \end{equation*} where \(B_{\delta}^{\star}(\widehat{P})=\arg\max_{Q\in B_{\delta}(\widehat{P})}\E_{Q}[J_{\theta}(x,y)]\) and constant \(K\) only depends on $p$ and $n$. </div> <p>We remark that the above results are easily extended to the out-of-sample performance on the test set, via the triangle inequality.</p> <h2 id="conclusion">Conclusion</h2> <p>Our work contributes to the understanding of robustness of DNN classifiers. It also offers a wider viewpoint on the question of robustness and naturally links the questions of adversarial attacks, out-of-sample performance, out-of-distribution performance and Knightian uncertainty. By introducing a first-order Adversarial Attack (AA) algorithm, we not only encompass existing methods like FGSM and PGD but also offer new insights into distributional threat models. We believe our research opens up many avenues for future work, especially training robust neural networks under distributional threat models.</p>]]></content><author><name></name></author><category term="expositions"/><category term="distributionally robust optimization"/><category term="adversarial attack"/><summary type="html"><![CDATA[A paper accepted for NeurIPS 2023.]]></summary></entry><entry><title type="html">Continuity of Covariance Operator</title><link href="https://yifanjiang233.github.io/blog/2023/cont_cov/" rel="alternate" type="text/html" title="Continuity of Covariance Operator"/><published>2023-04-28T00:00:00+01:00</published><updated>2023-04-28T00:00:00+01:00</updated><id>https://yifanjiang233.github.io/blog/2023/cont_cov</id><content type="html" xml:base="https://yifanjiang233.github.io/blog/2023/cont_cov/"><![CDATA[<p>We start with Kolmogorov continuity theorem which lies in the central of the continuous martingale theory.</p> <div class="thm" text="Kolmogorov"> Let \(\{X_{t}\}_{t\geq 0}\) be a real-valued stochastic process such that there exist \(\alpha, \beta , K\) satisfying \begin{equation*} \E \bigl[|X_{t}-X_{s}|^{\alpha}\bigr]\leq K |t-s|^{1+\beta}, \end{equation*} for any \(t,s\geq 0\). Then \(X\) has a \(\gamma\)–Hölder continuous modification for any \(0 &lt;\gamma &lt;\beta/\alpha \). </div> <p>By noticing the ultracontractivity of Gaussion measures, we immediately derive that fractional Brownian motion $ B^{H} $ has a $\gamma$–Hölder modification where $0&lt; \gamma &lt; H$. More generally, we can show for any centered random Gaussian field $\{X(x)\}_{x\in\mathbb{R}^{d}}$ with covariance $C$ has a $\gamma$–Hölder modification where $0&lt; \gamma &lt; H$ if \begin{equation} \label{eq-C} C(x,x)+C(y,y)-2C(x,y)\leq K|x-y|^{2H}. \end{equation} Here, $C$ is given by $C(x,y)=\E[X(x)X(y)]$. This, of course, implies the continuity of $C$. But, can we circumvent using Kolmogorov theorem?</p> <div class="prop"> Let \(C:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}\) be symmetric, locally bounded, and positive definite in the sense that for any finite collection \(\{x_{i}\}_{i=1}^{m}\) in \(\mathbb{R}^{d}\), the martix \(M_{ij}=C(x_{i},x_{j})\) is positive definite. Then \eqref{eq-C} implies the jointly continuity of \(C\). </div> <div class="proof"> By assmuption we take positive definite matrix \begin{equation*} M= \begin{pmatrix} C(y,y) &amp;C(y,x_{1}) &amp; C(y,x_{2}) \\ C(x_{1},y)&amp;C(x_{1},x_{1}) &amp; C(x_{1},x_{2}) \\ C(x_{2},y)&amp; C(x_{2},x_{1}) &amp; C(x_{2},x_{2}) \end{pmatrix}\end{equation*} and vecter \begin{equation*} \alpha =\begin{pmatrix} C(x_{1},x_{1})+C(x_{2},x_{2})-2C(x_{1},x_{2}) \\ C(x_{2},y)-C(x_{1},y)\\ C(x_{1},y)-C(x_{2},y)\end{pmatrix}.\end{equation*} Then we notice $\alpha^{\intercal} M\alpha \geq 0$ is equivalent to \begin{equation*} [C(x_1,y)-C(x_2,y)]^2 \leq 4 C(y,y)[C(x_{1},x_{1})+C(x_{2},x_{2})-2C(x_{1},x_{2})]. \end{equation*} By triangle inequality we conclude the proof. </div>]]></content><author><name></name></author><category term="brain teasers"/><category term="Gassuian random field"/><summary type="html"><![CDATA[We start with Kolmogorov continuity theorem which lies in the central of the continuous martingale theory.]]></summary></entry><entry><title type="html">Remote Server Cheatsheet</title><link href="https://yifanjiang233.github.io/blog/2023/server_cheatsheat/" rel="alternate" type="text/html" title="Remote Server Cheatsheet"/><published>2023-04-20T00:00:00+01:00</published><updated>2023-04-20T00:00:00+01:00</updated><id>https://yifanjiang233.github.io/blog/2023/server_cheatsheat</id><content type="html" xml:base="https://yifanjiang233.github.io/blog/2023/server_cheatsheat/"><![CDATA[<h2 id="jupyter-notebook">Jupyter Notebook</h2> <ol> <li> <p>Login your remote server.</p> <div class="language-ssh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ssh</span> username@server
</code></pre></div> </div> </li> <li> <p>Create a remote Jupyter notebook on the server.</p> <div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jupyter</span><span class="w"> </span><span class="nx">notebook</span><span class="w"> </span><span class="nt">--no-browser</span><span class="w"> </span><span class="nt">--port</span><span class="o">=</span><span class="mi">8080</span><span class="w">
</span></code></pre></div> </div> <p>Alternatively, you can define a macro in your configuration file <code class="language-plaintext highlighter-rouge">.bashrc</code> on the remote server.</p> <div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">function</span><span class="w"> </span><span class="nf">nb</span><span class="p">(){</span><span class="w">
    </span><span class="n">jupyter</span><span class="w"> </span><span class="nx">notebook</span><span class="w"> </span><span class="nt">--no-browser</span><span class="w"> </span><span class="nt">--port</span><span class="o">=</span><span class="mi">8080</span><span class="w">
 </span><span class="p">}</span><span class="w">
</span></code></pre></div> </div> </li> <li> <p>Open a new local terminal and connect to the remote Jupyter notebook</p> <div class="language-ssh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ssh</span> -L <span class="m">8080</span>:localhost:8080 username@server
</code></pre></div> </div> </li> <li> <p>Open <a href="https://localhost:8080">https://localhost:8080</a> in your browser.</p> </li> </ol> <h2 id="ssh">SSH</h2> <p>It is quite tedious to repeat above commands every time. Here’s how you can configure your own SSH profile to make it easier to connect to your remote server:</p> <ol> <li> <p>Add the following profile to your <code class="language-plaintext highlighter-rouge">.ssh/config</code> file.</p> <div class="language-ssh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">Host</span> myhost
    <span class="k">Hostname</span> server
    <span class="k">User</span> username
    <span class="k">Port</span> myport
    <span class="k">LocalForward</span> <span class="m">8080</span> localhost:8080
</code></pre></div> </div> </li> <li> <p>Connect and listen to the remote server simply through <code class="language-plaintext highlighter-rouge">ssh myhost</code>.</p> </li> </ol> <h2 id="tmux">Tmux</h2> <p>Tmux is a “terminal multiplexer”, it enables a number of terminals (or windows) to be accessed and controlled from a single terminal.</p> <ol> <li>Install <a href="https://github.com/tmux-plugins/tpm">Tmux Plugin Manager</a> and <a href="https://github.com/tmux-plugins/tmux-resurrect">Tmux Resurrect</a>.</li> <li> <p>Start a new session with the name <em>mysession</em>.</p> <div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tmux</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nt">-s</span><span class="w"> </span><span class="nx">mysession</span><span class="w">
</span></code></pre></div> </div> </li> <li>Save a session by <code class="language-plaintext highlighter-rouge">Ctrl-b + Ctrl-s</code> and restore a session by <code class="language-plaintext highlighter-rouge">Ctrl-b + Crtl-r</code>.</li> <li> <p>Show all sessions.</p> <div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tmux</span><span class="w"> </span><span class="nx">list-session</span><span class="w">
</span></code></pre></div> </div> </li> <li> <p>Kill session <em>mysession</em>.</p> <div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tmux</span><span class="w"> </span><span class="nx">kill-session</span><span class="w"> </span><span class="nt">-t</span><span class="w"> </span><span class="nx">mysession</span><span class="w">
</span></code></pre></div> </div> </li> </ol>]]></content><author><name></name></author><category term="codes"/><category term="python"/><category term="tmux"/><summary type="html"><![CDATA[Cheatsheet of Jupyter, tmux, etc.]]></summary></entry><entry><title type="html">Sensitivity of Robust Optimization over an Adapted Wasserstein Ambiguity Set</title><link href="https://yifanjiang233.github.io/blog/2022/AWDRO/" rel="alternate" type="text/html" title="Sensitivity of Robust Optimization over an Adapted Wasserstein Ambiguity Set"/><published>2022-09-05T00:00:00+01:00</published><updated>2022-09-05T00:00:00+01:00</updated><id>https://yifanjiang233.github.io/blog/2022/AWDRO</id><content type="html" xml:base="https://yifanjiang233.github.io/blog/2022/AWDRO/"><![CDATA[<h4 id="abstract">Abstract</h4> <p>In this talk, we consider the sensitivity of distributionally robust optimization problem under an adapted Wasserstein perturbation. We extend the classical results in a static setting to the dynamic multi-period setting. Under mild conditions, we give an explicit expression for the first order approximation to the value function. An optimization problem with a cost of weak type will also be discussed.</p> <p>See <a href="/assets/pdf/AWDRO.pdf">slides</a> for more details.</p>]]></content><author><name></name></author><category term="talks"/><category term="optimal transport"/><category term="robust optimization"/><summary type="html"><![CDATA[A talk given at London–Oxford–Warwick Mathematical Finance Workshop]]></summary></entry><entry><title type="html">Scalar Conservation Laws with Random Initial Data</title><link href="https://yifanjiang233.github.io/blog/2022/scalar_conservation_law/" rel="alternate" type="text/html" title="Scalar Conservation Laws with Random Initial Data"/><published>2022-08-25T00:00:00+01:00</published><updated>2022-08-25T00:00:00+01:00</updated><id>https://yifanjiang233.github.io/blog/2022/scalar_conservation_law</id><content type="html" xml:base="https://yifanjiang233.github.io/blog/2022/scalar_conservation_law/"><![CDATA[<h4 id="abstract">Abstract</h4> <p>In this talk, instead of considering the pathwise property of the entropy solution of a scalar conservation law, we are interested in the (probability) law of the solution. In 2010, Menon and Srinivasan conjectured that a certain class (spectrally negative) of Markov processes (in \(x\)) is preserved by the entropy solution and proposed the evolution (in \(t\)) of the generator of the solution. Bertoin and Sinai characterized Burgers’ equation with Brownian initial data; Rezakhanlou and Kaspar verified this conjecture for bounded spectrally negative Lévy processes for general scalar conservation laws. We will discuss their proofs and show how probability theory intertwines with nonlinear PDEs.</p> <p>See <a href="/assets/pdf/SCL_with_random_data.pdf">slides</a> for more details.</p>]]></content><author><name></name></author><category term="talks"/><category term="scalar conservation law"/><category term="shock wave"/><summary type="html"><![CDATA[A talk given at Nonlinear PDE Seminar]]></summary></entry><entry><title type="html">Robust Analysis via Optimal Transport Methods</title><link href="https://yifanjiang233.github.io/blog/2022/COT/" rel="alternate" type="text/html" title="Robust Analysis via Optimal Transport Methods"/><published>2022-04-06T00:00:00+01:00</published><updated>2022-04-06T00:00:00+01:00</updated><id>https://yifanjiang233.github.io/blog/2022/COT</id><content type="html" xml:base="https://yifanjiang233.github.io/blog/2022/COT/"><![CDATA[<h4 id="abstract">Abstract</h4> <p>This is an introductory talk on the causal optimal transport problem and its application to the robust analysis. We discuss the notion of causality under both Monge’s and Kantorovich’s formulation. We introduce the adapted Wasserstein metric arising from the causal optimal transport and compare it to the classical Wasserstein distance. A distibutionally robust optimization problem over an adapted Wasserstein ambiguity set is considered. We give an explicit formula of the sensitivity to the model uncertainty.</p> <p>See <a href="/assets/pdf/OT.pdf">slides</a> for more details.</p>]]></content><author><name></name></author><category term="talks"/><category term="optimal transport"/><category term="robust optimization"/><summary type="html"><![CDATA[A talk given at CDT spring retreat]]></summary></entry></feed>